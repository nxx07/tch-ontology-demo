#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TCMå¤šæºæ•°æ®å¤„ç†è„šæœ¬
ç”¨äºä»å¤šä¸ªMySQLæ•°æ®åº“ä¸­æå–å¹¶å¤„ç†ä¸­åŒ»æœ¯è¯­æ•°æ®ï¼ŒåŸºäºfield_mapping.csvçš„æ˜ å°„å…³ç³»

ä¸»è¦åŠŸèƒ½:
1. è¯»å–field_mapping.csvè·å–å­—æ®µæ˜ å°„å…³ç³»
2. ä»å¤šä¸ªæ•°æ®åº“(symap, herb2_0, etcm2_0)æå–æ•°æ®
3. æ ¹æ®æ¨¡æ¿æ ¼å¼å¤„ç†æ•°æ®
4. è¿½åŠ åˆ°ç°æœ‰çš„ä¸´æ—¶IDæ–‡ä»¶ä¸­ï¼Œç¡®ä¿IDå”¯ä¸€æ€§
5. æ”¯æŒpatternã€symptomã€herbã€formulaå››ç§æ•°æ®ç±»å‹

åˆ›å»ºæ—¶é—´: 2025.11.12
ç‰ˆæœ¬: 1.0
"""

import pymysql
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import csv
import re


# MySQLæ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIGS = {
    'symap': {
        'host': 'eggabc.site',
        'port': 19104,
        'user': 'tcm',
        'password': 'tcm@123',
        'database': 'symap',
        'charset': 'utf8mb4'
    },
    'herb2_0': {
        'host': 'eggabc.site',
        'port': 19104,
        'user': 'tcm',
        'password': 'tcm@123',
        'database': 'herb2_0',
        'charset': 'utf8mb4'
    },
    'etcm2_0': {
        'host': 'eggabc.site',
        'port': 19104,
        'user': 'tcm',
        'password': 'tcm@123',
        'database': 'etcm2_0',
        'charset': 'utf8mb4'
    }
}

# ä¸´æ—¶IDé…ç½®
TEMP_ID_CONFIGS = {
    'pattern': {
        'prefix': 'TmpTCH:PATTERN_',
        'start_number': 10000,
        'padding': 5
    },
    'symptom': {
        'prefix': 'TmpTCH:SYMPTOM_',
        'start_number': 20000,
        'padding': 5
    },
    'herb': {
        'prefix': 'TmpTCH:HERB_',
        'start_number': 100000,
        'padding': 6
    },
    'formula': {
        'prefix': 'TmpTCH:FORMULA_',
        'start_number': 400000,
        'padding': 6
    }
}

# æ–‡ä»¶è·¯å¾„
BASE_DIR = Path(__file__).parent
FIELD_MAPPING_FILE = BASE_DIR / 'field_mapping.csv'
OUTPUT_DIR = BASE_DIR.parent.parent.parent / 'data' / 'data_temp_ids'


class MultiSourceDataProcessor:
    """å¤šæºæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.connections = {}
        self.field_mappings = {}
        self.existing_data = {}
        self.current_temp_numbers = {}
        self.used_temp_ids = {}
        
        # åˆå§‹åŒ–æ¯ä¸ªæ•°æ®ç±»å‹çš„ä¸´æ—¶IDè®¡æ•°å™¨
        for data_type in TEMP_ID_CONFIGS.keys():
            self.current_temp_numbers[data_type] = TEMP_ID_CONFIGS[data_type]['start_number']
            self.used_temp_ids[data_type] = set()
    
    def connect_db(self, db_name: str):
        """è¿æ¥æŒ‡å®šçš„MySQLæ•°æ®åº“"""
        if db_name in self.connections:
            return self.connections[db_name]
        
        try:
            config = DB_CONFIGS[db_name]
            connection = pymysql.connect(
                host=config['host'],
                port=config['port'],
                user=config['user'],
                password=config['password'],
                database=config['database'],
                charset=config['charset'],
                connect_timeout=30,  # å¢åŠ è¿æ¥è¶…æ—¶
                read_timeout=300,     # å¢åŠ è¯»å–è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰
                write_timeout=300     # å¢åŠ å†™å…¥è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰
            )
            self.connections[db_name] = connection
            print(f"âœ… è¿æ¥åˆ°æ•°æ®åº“ {db_name} æˆåŠŸ")
            return connection
        except Exception as e:
            print(f"âŒ è¿æ¥æ•°æ®åº“ {db_name} å¤±è´¥: {e}")
            raise
    
    def close_connections(self):
        """å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
        for db_name, conn in self.connections.items():
            if conn:
                conn.close()
                print(f"âœ… æ•°æ®åº“ {db_name} è¿æ¥å·²å…³é—­")
    
    def load_field_mappings(self):
        """åŠ è½½å­—æ®µæ˜ å°„é…ç½®"""
        print("\n" + "="*70)
        print("ğŸ“¥ åŠ è½½å­—æ®µæ˜ å°„é…ç½®")
        print("="*70)
        
        try:
            df = pd.read_csv(FIELD_MAPPING_FILE)
            
            # æŒ‰data_typeåˆ†ç»„
            for data_type in df['data_type'].unique():
                type_mappings = df[df['data_type'] == data_type]
                
                # æŒ‰æ•°æ®åº“å’Œè¡¨åˆ†ç»„
                db_table_groups = {}
                for _, row in type_mappings.iterrows():
                    db = row['database']
                    table = row['table']
                    key = f"{db}.{table}"
                    
                    if key not in db_table_groups:
                        db_table_groups[key] = {
                            'database': db,
                            'table': table,
                            'mappings': []
                        }
                    
                    db_table_groups[key]['mappings'].append({
                        'template_field': row['template_field'],
                        'db_field': row['db_field'],
                        'notes': row['notes'] if pd.notna(row['notes']) else ''
                    })
                
                self.field_mappings[data_type] = db_table_groups
            
            print(f"âœ… æˆåŠŸåŠ è½½å­—æ®µæ˜ å°„é…ç½®")
            for data_type, groups in self.field_mappings.items():
                print(f"   - {data_type}: {len(groups)} ä¸ªæ•°æ®æº")
            
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å­—æ®µæ˜ å°„é…ç½®å¤±è´¥: {e}")
            return False
    
    def load_existing_data(self, data_type: str):
        """åŠ è½½ç°æœ‰çš„ä¸´æ—¶IDæ•°æ®"""
        output_file = OUTPUT_DIR / f'{data_type}_data_temp_ids.csv'
        
        if output_file.exists():
            try:
                df = pd.read_csv(output_file)
                self.existing_data[data_type] = df
                
                # æ”¶é›†å·²ä½¿ç”¨çš„ä¸´æ—¶ID
                for temp_id in df['tch_id']:
                    self.used_temp_ids[data_type].add(temp_id)
                    
                    # æ›´æ–°å½“å‰ä¸´æ—¶IDè®¡æ•°å™¨
                    if temp_id.startswith(TEMP_ID_CONFIGS[data_type]['prefix']):
                        try:
                            id_num = int(temp_id.replace(TEMP_ID_CONFIGS[data_type]['prefix'], ''))
                            if id_num >= self.current_temp_numbers[data_type]:
                                self.current_temp_numbers[data_type] = id_num + 1
                        except:
                            pass
                
                print(f"âœ… å·²åŠ è½½ç°æœ‰ {data_type} æ•°æ®: {len(df)} æ¡è®°å½•")
                print(f"   ä¸‹ä¸€ä¸ªä¸´æ—¶IDå°†ä» {TEMP_ID_CONFIGS[data_type]['prefix']}{str(self.current_temp_numbers[data_type]).zfill(TEMP_ID_CONFIGS[data_type]['padding'])} å¼€å§‹")
                return True
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ç°æœ‰ {data_type} æ•°æ®å¤±è´¥: {e}")
                return False
        else:
            print(f"â„¹ï¸  {data_type} æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            self.existing_data[data_type] = None
            return True
    
    def allocate_temp_id(self, data_type: str) -> str:
        """ä¸ºæŒ‡å®šæ•°æ®ç±»å‹åˆ†é…æ–°çš„ä¸´æ—¶ID"""
        prefix = TEMP_ID_CONFIGS[data_type]['prefix']
        padding = TEMP_ID_CONFIGS[data_type]['padding']
        
        while True:
            temp_id = f"{prefix}{str(self.current_temp_numbers[data_type]).zfill(padding)}"
            if temp_id not in self.used_temp_ids[data_type]:
                break
            self.current_temp_numbers[data_type] += 1
        
        self.used_temp_ids[data_type].add(temp_id)
        self.current_temp_numbers[data_type] += 1
        
        return temp_id
    
    def extract_field_names_from_expression(self, expression: str) -> List[str]:
        """
        ä»å­—æ®µè¡¨è¾¾å¼ä¸­æå–æ‰€æœ‰æ•°æ®åº“å­—æ®µå
        ä¾‹å¦‚: "SymMap2.0 SMSY"+Syndrome_id -> [Syndrome_id]
        ä¾‹å¦‚: Herb_id;SymMap_id;"TCMID: "+TCMID_id -> [Herb_id, SymMap_id, TCMID_id]
        """
        if not expression:
            return []
        
        field_names = []
        
        # åˆ†å‰²è¡¨è¾¾å¼ï¼ˆæŒ‰åˆ†å·æˆ–åŠ å·ï¼‰
        # é¦–å…ˆæå–æ‰€æœ‰å¼•å·åŒ…è£¹çš„éƒ¨åˆ†
        quoted_parts = re.findall(r'"[^"]*"', expression)
        # ä¸´æ—¶æ›¿æ¢å¼•å·éƒ¨åˆ†ä¸ºå ä½ç¬¦
        temp_expr = expression
        for i, quoted in enumerate(quoted_parts):
            temp_expr = temp_expr.replace(quoted, f'__QUOTED_{i}__', 1)
        
        # åˆ†å‰²è¡¨è¾¾å¼
        parts = re.split(r'[;+]', temp_expr)
        
        for part in parts:
            part = part.strip()
            # è·³è¿‡å ä½ç¬¦ï¼ˆå¼•å·å†…å®¹ï¼‰
            if part.startswith('__QUOTED_'):
                continue
            # è·³è¿‡ç©ºå­—ç¬¦ä¸²
            if not part:
                continue
            # è¿™åº”è¯¥æ˜¯ä¸€ä¸ªå­—æ®µå
            field_names.append(part)
        
        return field_names
    
    def parse_composite_field(self, row: pd.Series, db_field: str) -> str:
        """
        è§£æå¤åˆå­—æ®µè¡¨è¾¾å¼
        ä¾‹å¦‚: Herb_id;SymMap_id;"TCMID: "+TCMID_id
        ä¾‹å¦‚: "SymMap2.0 SMSY"+Syndrome_id
        """
        if not db_field:
            return ''
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Šæ“ä½œç¬¦
        if ';' in db_field or '+' in db_field or '"' in db_field:
            result_parts = []
            
            # åˆ†å‰²è¡¨è¾¾å¼ï¼ˆæŒ‰åˆ†å·ï¼‰
            if ';' in db_field:
                segments = db_field.split(';')
            else:
                segments = [db_field]
            
            for segment in segments:
                segment = segment.strip()
                
                # å¤„ç†æ¯ä¸ªæ®µï¼ˆå¯èƒ½åŒ…å«åŠ å·è¿æ¥ï¼‰
                if '+' in segment:
                    # åˆ†å‰²åŠ å·è¿æ¥çš„éƒ¨åˆ†
                    sub_parts = segment.split('+')
                    combined = ''
                    for sub_part in sub_parts:
                        sub_part = sub_part.strip()
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å¼•å·åŒ…è£¹çš„å­—ç¬¦ä¸²
                        if sub_part.startswith('"') and sub_part.endswith('"'):
                            # å»æ‰å¼•å·ï¼Œæ·»åŠ åˆ°combined
                            combined += sub_part[1:-1]
                        else:
                            # è¿™æ˜¯ä¸€ä¸ªå­—æ®µåï¼Œä»rowä¸­è·å–å€¼
                            value = row.get(sub_part, '')
                            if pd.notna(value) and str(value).strip():
                                combined += str(value)
                    
                    if combined:
                        result_parts.append(combined)
                else:
                    # æ²¡æœ‰åŠ å·ï¼Œç›´æ¥å¤„ç†
                    if segment.startswith('"') and segment.endswith('"'):
                        # å¼•å·åŒ…è£¹çš„å­—ç¬¦ä¸²
                        result_parts.append(segment[1:-1])
                    else:
                        # æ™®é€šå­—æ®µå
                        value = row.get(segment, '')
                        if pd.notna(value) and str(value).strip():
                            result_parts.append(str(value))
            
            # ç”¨åˆ†å·è¿æ¥æ‰€æœ‰éƒ¨åˆ†
            return ';'.join(result_parts) if result_parts else ''
        else:
            # ç®€å•å­—æ®µï¼Œç›´æ¥è·å–å€¼
            value = row.get(db_field, '')
            return str(value) if pd.notna(value) and str(value).strip() else ''
    
    def extract_data_from_source(self, data_type: str, db_name: str, table: str, mappings: List[Dict]) -> List[Dict]:
        """ä»æŒ‡å®šæ•°æ®æºæå–æ•°æ®ï¼ˆä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢é¿å…è¶…æ—¶ï¼‰"""
        print(f"\nğŸ“¥ ä» {db_name}.{table} æå– {data_type} æ•°æ®")
        
        try:
            connection = self.connect_db(db_name)
            
            # æ„å»ºéœ€è¦æŸ¥è¯¢çš„å­—æ®µåˆ—è¡¨
            db_fields = set()
            for mapping in mappings:
                db_field = mapping['db_field']
                # ä½¿ç”¨helperæ–¹æ³•æå–å­—æ®µå
                field_names = self.extract_field_names_from_expression(db_field)
                db_fields.update(field_names)
            
            # æ„å»ºSQLæŸ¥è¯¢ - ç”¨åå¼•å·åŒ…è£¹å­—æ®µåä»¥é¿å…ä¿ç•™å­—å†²çª
            fields_str = ', '.join(f'`{field}`' for field in db_fields)
            
            # åˆ†æ‰¹æŸ¥è¯¢ï¼Œæ¯æ¬¡æœ€å¤š2000æ¡è®°å½•ï¼ˆå‡å°æ‰¹é‡ä»¥é¿å…è¶…æ—¶ï¼‰
            batch_size = 2000
            offset = 0
            all_rows = []
            
            while True:
                query = f"SELECT {fields_str} FROM {table} LIMIT {batch_size} OFFSET {offset}"
                
                try:
                    # æ‰§è¡ŒæŸ¥è¯¢
                    with connection.cursor() as cursor:
                        cursor.execute(query)
                        rows = cursor.fetchall()
                        
                        if not rows:
                            break
                        
                        all_rows.extend(rows)
                        offset += batch_size
                        print(f"   å·²æå– {len(all_rows)} æ¡è®°å½•...")
                        
                        # å¦‚æœè¿”å›çš„è®°å½•å°‘äºbatch_sizeï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€æ‰¹
                        if len(rows) < batch_size:
                            break
                            
                except pymysql.err.OperationalError as e:
                    if '2013' in str(e):  # Lost connection
                        print(f"   âš ï¸  è¿æ¥æ–­å¼€ï¼Œå°è¯•é‡æ–°è¿æ¥...")
                        # å…³é—­æ—§è¿æ¥
                        if db_name in self.connections:
                            try:
                                self.connections[db_name].close()
                            except:
                                pass
                            del self.connections[db_name]
                        
                        # é‡æ–°è¿æ¥
                        connection = self.connect_db(db_name)
                        
                        # é‡è¯•å½“å‰æ‰¹æ¬¡
                        with connection.cursor() as cursor:
                            cursor.execute(query)
                            rows = cursor.fetchall()
                            
                            if not rows:
                                break
                            
                            all_rows.extend(rows)
                            offset += batch_size
                            print(f"   å·²æå– {len(all_rows)} æ¡è®°å½•ï¼ˆé‡è¿åç»§ç»­ï¼‰...")
                            
                            if len(rows) < batch_size:
                                break
                    else:
                        raise
            
            # æ„å»ºDataFrame
            df = pd.DataFrame(all_rows, columns=list(db_fields))
            
            print(f"âœ… æˆåŠŸæå– {len(df)} æ¡åŸå§‹æ•°æ®")
            
            # è½¬æ¢ä¸ºæ¨¡æ¿æ ¼å¼
            processed_records = []
            for idx, row in df.iterrows():
                record = {
                    'tch_id': self.allocate_temp_id(data_type),
                    'data_category': data_type
                }
                
                # æ ¹æ®æ˜ å°„å¡«å……å­—æ®µ
                for mapping in mappings:
                    template_field = mapping['template_field']
                    db_field = mapping['db_field']
                    
                    # è§£æå­—æ®µå€¼
                    value = self.parse_composite_field(row, db_field)
                    record[template_field] = value
                
                # æ·»åŠ æ ‡å‡†å­—æ®µ
                if 'date_accessed' not in record:
                    record['date_accessed'] = datetime.now().strftime('%Y-%m-%d')
                
                if 'notes' not in record:
                    record['notes'] = f'Source: {db_name}.{table}'
                else:
                    record['notes'] += f'; Source: {db_name}.{table}'
                
                processed_records.append(record)
            
            print(f"âœ… æˆåŠŸå¤„ç† {len(processed_records)} æ¡è®°å½•")
            return processed_records
                
        except Exception as e:
            print(f"âŒ ä» {db_name}.{table} æå–æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def process_data_type(self, data_type: str):
        """å¤„ç†æŒ‡å®šæ•°æ®ç±»å‹çš„æ‰€æœ‰æ•°æ®æº"""
        print("\n" + "="*70)
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {data_type.upper()} æ•°æ®")
        print("="*70)
        
        # åŠ è½½ç°æœ‰æ•°æ®
        self.load_existing_data(data_type)
        
        # è·å–è¯¥æ•°æ®ç±»å‹çš„æ‰€æœ‰æ•°æ®æº
        if data_type not in self.field_mappings:
            print(f"âš ï¸  æœªæ‰¾åˆ° {data_type} çš„å­—æ®µæ˜ å°„é…ç½®")
            return
        
        all_new_records = []
        
        # ä»æ¯ä¸ªæ•°æ®æºæå–æ•°æ®
        for source_key, source_info in self.field_mappings[data_type].items():
            db_name = source_info['database']
            table = source_info['table']
            mappings = source_info['mappings']
            
            records = self.extract_data_from_source(data_type, db_name, table, mappings)
            all_new_records.extend(records)
        
        if not all_new_records:
            print(f"âš ï¸  æœªæå–åˆ°æ–°çš„ {data_type} æ•°æ®")
            return
        
        # åˆå¹¶æ–°æ—§æ•°æ®
        new_df = pd.DataFrame(all_new_records)
        
        if self.existing_data[data_type] is not None:
            # è¿½åŠ åˆ°ç°æœ‰æ•°æ®
            combined_df = pd.concat([self.existing_data[data_type], new_df], ignore_index=True)
            print(f"âœ… è¿½åŠ  {len(new_df)} æ¡æ–°è®°å½•åˆ°ç°æœ‰ {len(self.existing_data[data_type])} æ¡è®°å½•")
        else:
            combined_df = new_df
            print(f"âœ… åˆ›å»ºæ–°æ–‡ä»¶ï¼ŒåŒ…å« {len(combined_df)} æ¡è®°å½•")
        
        # ä¿å­˜æ•°æ®
        output_file = OUTPUT_DIR / f'{data_type}_data_temp_ids.csv'
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        combined_df.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"   æ€»è®°å½•æ•°: {len(combined_df)}")
        print(f"   æ–°å¢è®°å½•æ•°: {len(new_df)}")
        print(f"   æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.2f} KB")
    
    def process_all_data_types(self, data_types: List[str] = None):
        """å¤„ç†æ‰€æœ‰æˆ–æŒ‡å®šçš„æ•°æ®ç±»å‹"""
        if data_types is None:
            data_types = ['pattern', 'symptom', 'herb', 'formula']
        
        print("="*70)
        print("TCMå¤šæºæ•°æ®å¤„ç†ç¨‹åº".center(70))
        print("="*70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å¤„ç†æ•°æ®ç±»å‹: {', '.join(data_types)}")
        
        try:
            # åŠ è½½å­—æ®µæ˜ å°„
            if not self.load_field_mappings():
                return
            
            # å¤„ç†æ¯ä¸ªæ•°æ®ç±»å‹
            for data_type in data_types:
                self.process_data_type(data_type)
            
            print("\n" + "="*70)
            print("âœ… æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆ!".center(70))
            print("="*70)
            print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.close_connections()


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    processor = MultiSourceDataProcessor()
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ•°æ®ç±»å‹
    if len(sys.argv) > 1:
        data_types = sys.argv[1:]
        processor.process_all_data_types(data_types)
    else:
        # é»˜è®¤å¤„ç†æ‰€æœ‰æ•°æ®ç±»å‹
        processor.process_all_data_types()


if __name__ == '__main__':
    main()
